{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78c1eef-4b5b-4a8a-a910-01fbbb3a0097",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install PyPDF2==3.0.0\n",
    "!pip install cohere\n",
    "!pip install langchain langchain-cohere langchain-openai\n",
    "!pip install pytesseract\n",
    "!pip install tkinter\n",
    "!pip install pdf2image langsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57182921-ba36-44b2-a40e-0443b6acf546",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere.llms import Cohere\n",
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"COHERE_API_KEY\"] = \"2zPsEcvQ1tzELqmw0ORVcVEX51H5p7KXLrpaqmrz\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f26d431-761f-4b0c-b2e6-3e9b65f3a193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "\n",
    "def select_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename()\n",
    "\n",
    "    return file_path\n",
    "\n",
    "selected_file_path = select_file()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41151468-cfd4-44bb-acd3-3f2b3190545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PdfReader(file)\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            text += page.extract_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8238de-2091-49fc-817c-0a1ab00f3f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "\n",
    "def apply_ocr_to_pdf(pdf_path):\n",
    "    images = convert_from_path(pdf_path)\n",
    "    text = \"\"\n",
    "    for image in images:\n",
    "        text += pytesseract.image_to_string(image)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e1c28-d751-4b82-86b0-eabc17441262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to determine whether to extract text or apply OCR\n",
    "def process_pdf(pdf_path):\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    if len(text) < 100:\n",
    "        text = apply_ocr_to_pdf(pdf_path)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7413151-9f05-4d98-bc38-62081ec8e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "    Given the following context:\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    Please answer the following question based on the above context:\n",
    "    \n",
    "    {question}\n",
    "    \n",
    "    Your answer should be concise and focus on the most important details. Make sure the answer is in JSON format. If you don't know a specific value, keep it empty. The dates should be specified like this: 25th January 2011.\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8faf57c-64f2-41ed-b607-424da27d80a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_cohere import ChatCohere\n",
    "\n",
    "model = ChatCohere(model=\"command-r-plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a883a-79ea-4963-9e7a-56aa1ee824c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "# QA\n",
    "inputs = [\n",
    "    \"What are the names of the passengers?\",\n",
    "    \"What is the airline's name and flight number?\",\n",
    "    \"Can you provide me the flight routes along with the time?\",\n",
    "    \"Can you provide me with the ticket number of the passengers?\",\n",
    "    \"What are the flight duration for all the flights mentioned?\",\n",
    "    \"What are the baggage limits?\",\n",
    "    \"What are the date of birth of the passengers?\",\n",
    "    \"What are the ticket numbers?\"\n",
    "]\n",
    "\n",
    "outputs = [\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "qa_pairs = [{\"question\": q, \"answer\": a} for q, a in zip(inputs, outputs)]\n",
    "\n",
    "# Create dataset\n",
    "client = Client()\n",
    "dataset_name = \"ticket_test_evaluation_hallucination\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"QA pairs about ticket.\",\n",
    ")\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": q} for q in inputs],\n",
    "    outputs=[{\"answer\": a} for a in outputs],\n",
    "    dataset_id=dataset.id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b4300-99dc-4889-92fb-1230fc4f39a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = prompt_template | model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa6b16a-9e4b-4198-928f-b10946d84f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_context(example: dict):\n",
    "    context = process_pdf(selected_file_path)\n",
    "    result = chain.invoke({\"context\": context, \"question\": example[\"question\"]})\n",
    "    return {\"answer\": result, \"contexts\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2543a9-3a53-4471-8cac-4d945be83aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import LangChainStringEvaluator, evaluate\n",
    "\n",
    "answer_hallucination_evaluator = LangChainStringEvaluator(\n",
    "    \"labeled_score_string\",\n",
    "    config={\n",
    "        \"criteria\": {\n",
    "            \"accuracy\": \"\"\"Is the Assistant's Answer grounded in the Ground Truth documentation? A score of [[1]] means that the\n",
    "            Assistant answer contains is not at all based upon / grounded in the Groun Truth documentation. A score of [[5]] means \n",
    "            that the Assistant answer contains some information (e.g., a hallucination) that is not captured in the Ground Truth \n",
    "            documentation. A score of [[10]] means that the Assistant answer is fully based upon the in the Ground Truth documentation.\"\"\"\n",
    "        },\n",
    "        # If you want the score to be saved on a scale from 0 to 1\n",
    "        \"normalize_by\": 10,\n",
    "    },\n",
    "    prepare_data=lambda run, example: {\n",
    "        \"prediction\": run.outputs[\"answer\"],\n",
    "        \"reference\": run.outputs[\"contexts\"],\n",
    "        \"input\": example.inputs[\"question\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72b9976-0073-4112-aaff-fa5c5fb28a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"ticket_test_evaluation_hallucination\"\n",
    "experiment_results = evaluate(\n",
    "    answer_with_context,\n",
    "    data=dataset_name,\n",
    "    evaluators=[answer_hallucination_evaluator],\n",
    "    experiment_prefix=\"rag-qa-cohere-hallucination\",\n",
    "    # Any experiment metadata can be specified here\n",
    "    metadata={\n",
    "        \"variant\": \"ticket_test_evaluation_hallucination_cohere\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ccc569b-a03d-43a2-9cc0-b1ea761cd9db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
